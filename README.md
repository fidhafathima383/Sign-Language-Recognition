# Sign-Language-Recognition
This project builds a system to recognize and translate sign language gestures into readable text using computer vision and machine learning. It processes images or videos of hand gestures to map them to sign language meanings, aiming to bridge the communication gap for individuals with hearing or speech impairments.
Key Features:
Real-time recognition of sign language gestures
Uses a convolutional neural network (CNN) for gesture classification
Image preprocessing using OpenCV for hand segmentation and background removal
Expandable to other sign languages
Technologies Used:
Python
OpenCV
TensorFlow/Keras
Jupyter Notebook
